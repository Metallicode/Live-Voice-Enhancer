#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
HebrewCorrector.py

General Hebrew post-STT correction module with:
- Cleanup-only baseline
- Optional SymSpell word correction using a Hebrew word frequency list
- Optional bigram scoring + beam search using a bigram frequency list

Files expected (generated by your aux scripts):
- hebrew_freq.txt      (format: "word count")
- hebrew_bigrams.txt   (format: "word1 word2 count")

Usage (interactive tester):
  python HebrewCorrector.py
  python HebrewCorrector.py --wordlist hebrew_freq.txt
  python HebrewCorrector.py --wordlist hebrew_freq.txt --bigrams hebrew_bigrams.txt
"""

from __future__ import annotations

import argparse
import math
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

from rapidfuzz import fuzz

try:
    from symspellpy import SymSpell, Verbosity
except ImportError:
    SymSpell = None  # type: ignore
    Verbosity = None  # type: ignore


# -----------------------------
# Hebrew normalization helpers
# -----------------------------

HEB_LETTERS = r"א-ת"
NIQQUD_RE = re.compile(r"[\u0591-\u05C7]")  # cantillation + niqqud

FINAL_TO_BASE = {
    "ך": "כ",
    "ם": "מ",
    "ן": "נ",
    "ף": "פ",
    "ץ": "צ",
}

CONFUSIONS = [
    ("ב", "ו"),
    ("ו", "ב"),
    ("ט", "ת"),
    ("ת", "ט"),
    ("כ", "ח"),
    ("ח", "כ"),
    ("ס", "ש"),
    ("ש", "ס"),
]

BOOST_WORDS = {
    "אני","אתה","את","זה","מה","כן","לא","עוד","כאן","שם",
    "עכשיו","אחר כך","קודם","הקודם","הבא","הזאת","הזה",

    "בבקשה","אפשר","תודה","סליחה","רוצה","צריך","צריכה",
    "יכול","יכולה","תוכל","תוכלי","מבקש","מבקשת",

    "לשמוע","לראות","לדבר","לקרוא","לעזור","לקבל","לתת",
    "להביא","ללכת","לבוא","לעשות","להפסיק","להתחיל",
    "להפעיל","לכבות","לחזור",

    "שיר","מוזיקה","השיר","הפלייליסט","התחנה",
    "לנגן","תנגן","תשמיע","עצור","הפסק",
    "קדימה","אחורה","ווליום","חזק","חלש",

    "מים","כוס","בקבוק","אוכל","לאכול","לשתות",
    "רעב","צמא","חם","קר",

    "חבר","חברה","אמא","אבא","אח","אחות",
    "מישהו","מישהי","כולם","אף אחד",

    "עזרה","כואב","כאב","בעיה","בסדר","טוב","רע",
    "קשה","קל","לאט","מהר","שוב",

    "פה","שם","לשם","מפה","היום","מחר","אתמול",
    "עכשיו","אחר כך","מאוחר","מוקדם"
}





BASE_TO_FINAL = {v: k for k, v in FINAL_TO_BASE.items()}

WORD_RE = re.compile(rf"[{HEB_LETTERS}]+")


def strip_niqqud(text: str) -> str:
    return NIQQUD_RE.sub("", text)


def normalize_quotes(text: str) -> str:
    return (
        text.replace("“", '"')
        .replace("”", '"')
        .replace("״", '"')
        .replace("’", "'")
        .replace("‘", "'")
        .replace("׳", "'")
    )


def normalize_whitespace(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()


def reduce_repeated_chars(text: str) -> str:
    # "טובבבבב" -> "טוב"
    return re.sub(rf"([{HEB_LETTERS}])\1{{2,}}", r"\1", text)


def normalize_punctuation_spacing(text: str) -> str:
    # remove spaces before punctuation
    text = re.sub(r"\s+([,.:;!?])", r"\1", text)
    # add a space after punctuation if missing
    text = re.sub(r"([,.:;!?])([^\s])", r"\1 \2", text)
    return text


def final_letters_to_base(text: str) -> str:
    return "".join(FINAL_TO_BASE.get(ch, ch) for ch in text)


def base_to_final_simple(word: str) -> str:
    """Very naive readability tweak: if ends with base form that has final, switch to final."""
    if not word:
        return word
    last = word[-1]
    if last in BASE_TO_FINAL:
        return word[:-1] + BASE_TO_FINAL[last]
    return word


def is_hebrew_word(token: str) -> bool:
    return bool(re.fullmatch(rf"[{HEB_LETTERS}]+", token))


def generate_confusion_variants(word: str, limit: int = 12) -> List[str]:
    variants = {word}
    for a, b in CONFUSIONS:
        if a in word:
            variants.add(word.replace(a, b))
    # also try removing duplicated letters (already done globally, but helps per-word too)
    variants.add(re.sub(rf"([{HEB_LETTERS}])\1+", r"\1", word))
    out = list(variants)
    out.sort(key=len)
    return out[:limit]



# -----------------------------
# Data containers
# -----------------------------

@dataclass
class Candidate:
    text: str
    score: float
    notes: str


# -----------------------------
# Loading wordlist + bigrams
# -----------------------------

def load_word_counts(wordlist_path: str) -> Dict[str, int]:
    """
    Load "word count" lines into dict.
    We also normalize final letters to base in keys for consistency.
    """
    counts: Dict[str, int] = {}
    with open(wordlist_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split()
            if len(parts) != 2:
                continue
            w, c = parts
            w = final_letters_to_base(strip_niqqud(w))
            try:
                counts[w] = int(c)
            except ValueError:
                continue
    return counts


def load_bigram_counts(bigram_path: str) -> Dict[Tuple[str, str], int]:
    """
    Load "w1 w2 count" lines into dict.
    Normalize final letters to base.
    """
    bigrams: Dict[Tuple[str, str], int] = {}
    with open(bigram_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split()
            if len(parts) != 3:
                continue
            w1, w2, c = parts
            w1 = final_letters_to_base(strip_niqqud(w1))
            w2 = final_letters_to_base(strip_niqqud(w2))
            try:
                bigrams[(w1, w2)] = int(c)
            except ValueError:
                continue
    return bigrams


# -----------------------------
# Core Corrector
# -----------------------------

class HebrewCorrector:
    """
    General Hebrew correction designed for noisy STT.
    It avoids "hallucinating" by:
    - favoring small edits
    - using a dictionary + bigrams to choose likely sequences
    - returning multiple candidates
    """

    def __init__(
        self,
        wordlist_path: Optional[str] = None,
        bigram_path: Optional[str] = None,
        max_edit_distance: int = 2,
        prefix_length: int = 7,
    ):
        self.wordlist_path = wordlist_path
        self.bigram_path = bigram_path

        self.word_counts: Dict[str, int] = {}
        self.bigram_counts: Dict[Tuple[str, str], int] = {}

        self.symspell = None

        if wordlist_path:
            if SymSpell is None:
                raise RuntimeError("symspellpy is required when using --wordlist. pip install symspellpy")
            self.word_counts = load_word_counts(wordlist_path)
            self.symspell = SymSpell(max_dictionary_edit_distance=max_edit_distance, prefix_length=prefix_length)
            ok = self.symspell.load_dictionary(wordlist_path, term_index=0, count_index=1, separator=" ")
            if not ok:
                raise RuntimeError(f"Failed to load SymSpell dictionary from: {wordlist_path}")

        if bigram_path:
            self.bigram_counts = load_bigram_counts(bigram_path)

        # Precompute totals for smoothing
        self._unigram_total = sum(self.word_counts.values()) if self.word_counts else 0
        self._unigram_vocab = len(self.word_counts) if self.word_counts else 0

    # ---------- Cleanup ----------

    def cleanup(self, text: str) -> str:
        t = normalize_quotes(text)
        t = strip_niqqud(t)
        t = reduce_repeated_chars(t)
        t = normalize_punctuation_spacing(t)
        t = normalize_whitespace(t)
        return t

    # ---------- Tokenization ----------
    def tokenize(self, text: str) -> List[str]:
        # Keep punctuation as separators; we mainly care about Hebrew words.
        # Return "words" split by whitespace.
        return normalize_whitespace(text).split(" ")

    # ---------- Scoring ----------
    def _log_unigram(self, w_base: str) -> float:
        """
        Smoothed unigram log-prob.
        If no wordlist, return neutral score.
        """
        if not self.word_counts:
            return 0.0

        # add-one smoothing
        c = self.word_counts.get(w_base, 0)
        return math.log((c + 1) / (self._unigram_total + self._unigram_vocab))

    def _log_bigram(self, w1_base: str, w2_base: str) -> float:
        """
        Smoothed bigram log-prob with backoff.
        If no bigrams, return 0.
        """
        if not self.bigram_counts:
            return 0.0

        c12 = self.bigram_counts.get((w1_base, w2_base), 0)
        if c12 > 0:
            return 1.6 * math.log(c12 + 1)

        # unseen transition penalty (tunable)
        return -4.0

    # ---------- SymSpell suggestions ----------
    def _suggest_word(self, word: str, k: int) -> List[Tuple[str, float]]:
        if not self.symspell or not is_hebrew_word(word):
            return [(word, 0.0)]

        variants = generate_confusion_variants(final_letters_to_base(word), limit=12)

        best: Dict[str, float] = {}

        for q in variants:
            suggestions = self.symspell.lookup(
                q,
                Verbosity.CLOSEST,     # important: allow multiple close suggestions
                max_edit_distance=2,
                include_unknown=True,
            )

            for s in suggestions[: max(k, 1)]:
                term = s.term
                sim = fuzz.ratio(q, term)
                penalty = (100.0 - sim) / 18.0  # slightly less harsh than before
                best[term] = min(best.get(term, 1e9), penalty)

        # Always include original
        best[final_letters_to_base(word)] = min(best.get(final_letters_to_base(word), 1e9), 0.8)

        items = sorted(best.items(), key=lambda x: x[1])
        return items[:k]


    # ---------- Beam search sequence correction ----------
    def correct_with_beam(
        self,
        tokens: List[str],
        per_word_k: int = 3,
        beam_width: int = 6,
    ) -> List[Tuple[List[str], float]]:
        """
        Build best sequences using unigram + bigram scores + edit penalties.

        Returns list of (token_list_in_base_form, score) sorted best-first.
        Higher score is better (we use log-prob style with penalties).
        """
        # For each token, generate candidate words (base-form)
        cand_lists: List[List[Tuple[str, float]]] = []
        for tok in tokens:
            if is_hebrew_word(tok):
                cand_lists.append(self._suggest_word(tok, per_word_k))
            else:
                # non-Hebrew tokens: keep as-is, no penalty
                cand_lists.append([(tok, 0.0)])

        # Beam entries: (seq_tokens, score, last_hebrew_base_or_None)
        beam: List[Tuple[List[str], float, Optional[str]]] = [([], 0.0, None)]

        for idx, cand_words in enumerate(cand_lists):
            new_beam: List[Tuple[List[str], float, Optional[str]]] = []
            for seq, score, last in beam:
                for w, edit_penalty in cand_words:
                    seq2 = seq + [w]

                    # Score contribution
                    s2 = score

                    # If Hebrew word, apply unigram score and bigram transition

                    if is_hebrew_word(w):
                        w_base = final_letters_to_base(w)

                        # small positive bias for likely assistive vocabulary
                        if w_base in BOOST_WORDS:
                            s2 += 0.8

                        s2 += self._log_unigram(w_base)
                        if last is not None:
                            s2 += self._log_bigram(last, w_base)
                        last2 = w_base
                    else:
                        last2 = last

                    # Penalty for changing too much
                    s2 -= edit_penalty

                    new_beam.append((seq2, s2, last2))

            # Keep top beam_width
            new_beam.sort(key=lambda x: x[1], reverse=True)
            beam = new_beam[:beam_width]

        # Return without last-state
        out = [(seq, score) for (seq, score, _) in beam]
        out.sort(key=lambda x: x[1], reverse=True)
        return out

    def _postprocess_readability(self, text: str) -> str:
        """
        Convert end letters to final forms (naive), and re-normalize spaces/punct.
        """
        parts = text.split(" ")
        parts2 = []
        for p in parts:
            if is_hebrew_word(p):
                parts2.append(base_to_final_simple(p))
            else:
                parts2.append(p)
        return normalize_whitespace(normalize_punctuation_spacing(" ".join(parts2)))

    # ---------- Public API ----------
    def suggest(
        self,
        raw_text: str,
        n: int = 3,
        per_word_k: int = 3,
        beam_width: int = 6,
    ) -> List[Candidate]:
        base = self.cleanup(raw_text)

        # Always include cleanup-only candidate
        candidates: List[Candidate] = [Candidate(text=base, score=0.0, notes="cleanup")]

        # If no resources, stop here (still useful)
        if not self.symspell and not self.bigram_counts:
            return candidates[:n]

        tokens = self.tokenize(base)

        # Beam search (uses symspell if available; uses bigrams if available)
        beams = self.correct_with_beam(tokens, per_word_k=per_word_k, beam_width=beam_width)

        for seq, score in beams:
            txt = " ".join(seq)
            txt = self._postprocess_readability(txt)
            candidates.append(Candidate(text=txt, score=score, notes="beam(unigram+bigram+edit)"))

        # Deduplicate by text (keep best score)
        best: Dict[str, Candidate] = {}
        for c in candidates:
            if c.text not in best or c.score > best[c.text].score:
                best[c.text] = c

        out = list(best.values())
        out.sort(key=lambda x: x.score, reverse=True)

        # Keep top n
        return out[:n]


# -----------------------------
# CLI tester
# -----------------------------

def main():
    ap = argparse.ArgumentParser(description="HebrewCorrector interactive tester (general Hebrew + optional bigrams).")
    ap.add_argument("--wordlist", default=None, help="Path to hebrew_freq.txt (word count)")
    ap.add_argument("--bigrams", default=None, help="Path to hebrew_bigrams.txt (w1 w2 count)")
    ap.add_argument("-n", "--topn", type=int, default=5, help="How many candidates to show")
    ap.add_argument("--k", type=int, default=3, help="Per-word candidate count (SymSpell)")
    ap.add_argument("--beam", type=int, default=6, help="Beam width for sequence search")
    args = ap.parse_args()

    corr = HebrewCorrector(wordlist_path=args.wordlist, bigram_path=args.bigrams)

    print("HebrewCorrector interactive tester")
    print("Paste raw STT text and press Enter. Ctrl+C to exit.\n")
    if args.wordlist:
        print(f"Loaded wordlist: {args.wordlist}")
    if args.bigrams:
        print(f"Loaded bigrams: {args.bigrams}")
    print()

    while True:
        try:
            s = input("> ").strip()
            if not s:
                continue
            cands = corr.suggest(s, n=args.topn, per_word_k=args.k, beam_width=args.beam)
            for i, c in enumerate(cands, 1):
                # score is relative; higher better
                score_str = f"{c.score:.3f}" if isinstance(c.score, float) else str(c.score)
                print(f"{i}. {c.text}   [score={score_str}]   ({c.notes})")
            print()
        except KeyboardInterrupt:
            print("\nbye")
            return


if __name__ == "__main__":
    main()

